# üöÄ **Deploy Multi-Agent Platform with Ollama**

## ‚úÖ **Updated Configuration - Ollama Included**

You're absolutely right! Ollama is extensively used throughout the application:

- **LLM Provider**: Core LLM provider alongside OpenAI
- **Message Processing**: Chat and AI agent interactions
- **Provider Management**: Integrated with fallback logic
- **Configuration**: Dedicated environment variables and setup

## üîß **Updated Deployment Configuration**

### **Services Included:**
- ‚úÖ **Frontend** (React + TypeScript)
- ‚úÖ **Backend** (FastAPI + Python)
- ‚úÖ **Neo4j** (Graph Database)
- ‚úÖ **Ollama** (Local LLM Server) - **NOW INCLUDED**
- ‚úÖ **SQLite** (Local Storage)

### **Port Configuration:**
```
Frontend: 8086 -> 80
Backend:  8002 -> 8000
Neo4j:    7475 -> 7474 (HTTP), 7688 -> 7687 (Bolt)
Ollama:   11435 -> 11434
```

## üöÄ **Quick Deploy**

### **1. Deploy with Ollama**
```bash
# Use the updated deployment script
./deploy-custom-ports.sh

# Or manually with Docker Compose
docker-compose -f docker-compose-with-ollama.yml up -d --build
```

### **2. Access the Application**
```
Frontend:    http://localhost:8086
Backend API: http://localhost:8002
Neo4j:       http://localhost:7475
Ollama:      http://localhost:11435
```

### **3. Verify Ollama is Working**
```bash
# Check Ollama API
curl http://localhost:11435/api/tags

# Pull a model (first time setup)
docker exec multi-agent-ollama ollama pull llama2

# Test with a simple query
curl -X POST http://localhost:11435/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "llama2", "prompt": "Hello, how are you?"}'
```

## üéØ **Ollama Configuration**

### **Environment Variables:**
```bash
# Backend
OLLAMA_BASE_URL=http://ollama:11434

# Frontend
VITE_OLLAMA_BASE_URL=http://localhost:11435
```

### **Available Models:**
```bash
# Pull popular models
docker exec multi-agent-ollama ollama pull llama2
docker exec multi-agent-ollama ollama pull codellama
docker exec multi-agent-ollama ollama pull mistral
docker exec multi-agent-ollama ollama pull neural-chat

# List installed models
docker exec multi-agent-ollama ollama list
```

### **GPU Support (Optional):**
If you have NVIDIA GPU:
```yaml
# In docker-compose-with-ollama.yml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

## üîß **LLM Provider Configuration**

### **Frontend Configuration:**
The application supports multiple LLM providers:

1. **OpenAI** (if API key provided)
2. **Ollama** (local models)
3. **Fallback Logic**: Automatically switches between providers

### **Provider Selection:**
```typescript
// In the frontend, users can select:
// - OpenAI (requires API key)
// - Ollama (local, no API key needed)
// - Auto (tries OpenAI first, falls back to Ollama)
```

## üéÆ **Using Ollama in the Application**

### **1. Chat Interface:**
- Users can chat with AI agents using Ollama models
- No API key required for local models
- Faster response times (no external API calls)

### **2. AI Agent Automation:**
- Desktop automation agents use Ollama for natural language understanding
- Browser automation planning uses Ollama for task analysis
- Knowledge base queries can use Ollama for local processing

### **3. Workflow Processing:**
- Workflow orchestration can use Ollama for decision making
- Document processing and analysis
- Email response generation

## üìä **Performance Considerations**

### **Memory Requirements:**
- **Ollama Container**: 2-4GB RAM (depending on model)
- **Total System**: 16GB+ RAM recommended
- **Model Size**: 3-7GB per model (first download)

### **Storage Requirements:**
- **Ollama Models**: 10-50GB (depending on models installed)
- **Persistent Volume**: `ollama_data` for model storage

### **Network:**
- **Internal**: Ollama accessible within Docker network
- **External**: Ollama accessible on port 11435
- **API**: RESTful API compatible with OpenAI format

## üîç **Troubleshooting**

### **Ollama Not Starting:**
```bash
# Check logs
docker logs multi-agent-ollama

# Restart Ollama
docker restart multi-agent-ollama

# Check if models are available
docker exec multi-agent-ollama ollama list
```

### **Models Not Loading:**
```bash
# Pull a model manually
docker exec multi-agent-ollama ollama pull llama2

# Check model status
docker exec multi-agent-ollama ollama ps
```

### **API Connection Issues:**
```bash
# Test API connectivity
curl http://localhost:11435/api/tags

# Check if Ollama is accessible from backend
docker exec multi-agent-backend curl http://ollama:11434/api/tags
```

## üéâ **Benefits of Including Ollama**

### **‚úÖ Advantages:**
- **No API Costs**: Local models, no per-token charges
- **Privacy**: All processing happens locally
- **Offline Capability**: Works without internet
- **Custom Models**: Can use fine-tuned models
- **Fast Response**: No network latency
- **Unlimited Usage**: No rate limits

### **‚úÖ Use Cases:**
- **Development & Testing**: No API key needed
- **Privacy-Sensitive**: Local processing only
- **High Volume**: No rate limits
- **Custom Workflows**: Tailored model responses
- **Offline Operations**: Works without internet

## üöÄ **Ready to Deploy**

```bash
# Deploy with Ollama included
./deploy-custom-ports.sh

# Access the application
Frontend: http://localhost:8086
Ollama:   http://localhost:11435

# Pull your first model
docker exec multi-agent-ollama ollama pull llama2
```

**Now you have a complete Multi-Agent Platform with local LLM capabilities via Ollama!** üéâ

The application will automatically use Ollama for AI agent interactions, automation planning, and natural language processing - all running locally on your server.
