# ğŸ¤– LLM PROVIDER SELECTION - EXACT FLOW

## ğŸ¯ **HOW PROVIDERS ARE SELECTED (STEP-BY-STEP)**

Let me trace the **EXACT code path** from user message to LLM selection:

---

## ğŸ”„ **COMPLETE SELECTION FLOW**

### **User sends message: "Analyze the quarterly sales data"**

---

### **STEP 1: Message Enters System**

**File:** `src/services/chat/ChatProcessor.ts` (Line 75)

```typescript
// Route to Orchestrator
const orchestratorResponse = await this.orchestrator.processRequest({ 
  message,  // "Analyze the quarterly sales data"
  agent,    // Finance Agent
  userId,
  conversationHistory,
  // ...
});
```

---

### **STEP 2: Orchestrator Routes to Agent**

**File:** `src/services/orchestrator/OrchestratorAgent.ts` (Lines 442-467)

```typescript
// Get agent instance
const agentInstance = await AgentFactory.getInstance().getAgentInstance(agent.id);

// Call RAG-powered response
const response = await agentInstance.generateResponseWithRAG(
  message,            // "Analyze the quarterly sales data"
  formattedHistory,
  userId,
  agentContext
);
```

---

### **STEP 3: Agent Detects Required Skill**

**File:** `src/services/agent/BaseAgent.ts` (Lines 550-580)

```typescript
protected async generateResponseWithRAG(
  userMessage: string,  // "Analyze the quarterly sales data"
  conversationHistory,
  userId,
  context
): Promise<string> {
  
  // Build RAG context...
  const ragContext = await this.buildRAGContext(...);
  
  // Build messages...
  const messages: LLMMessage[] = [...];
  
  // âœ¨ DETECT SKILL FROM CONTEXT
  const skillName = this.detectSkillFromContext(context);
  //    â†‘
  //    Returns: 'data_analysis' or 'financial_analysis'
  
  // âœ¨ EXECUTE WITH SELECTED LLM
  const response = await this.executeLLM(messages, skillName);
  //                                                â†‘
  //                                This is where provider selection happens!
  
  return response;
}
```

---

### **STEP 4: Detect Skill from Context**

**File:** `src/services/agent/BaseAgent.ts` (Lines 319-329)

```typescript
private detectSkillFromContext(context: AgentContext): string | undefined {
  // Check if context has a specific action that maps to a skill
  if (context.action) {
    return context.action;
  }
  
  // Default to agent's primary skill
  return this.config.skills[0]?.name;
}
```

**Result:** `skillName = 'data_analysis'` (or similar)

---

### **STEP 5: Execute LLM with Skill-Based Selection**

**File:** `src/services/agent/BaseAgent.ts` (Lines 287-318)

```typescript
protected async executeLLM(
  messages: LLMMessage[],
  skillName?: string     // â† 'data_analysis'
): Promise<string> {
  
  // 1ï¸âƒ£ FIND THE SKILL OBJECT
  const skill = skillName 
    ? this.config.skills.find(s => s.name === skillName)
    : undefined;
  
  // skill = { name: 'data_analysis', level: 5, config: {...} }
  
  // 2ï¸âƒ£ SELECT LLM FOR THIS SKILL
  const llmConfig = skill 
    ? this.llmRouter.selectLLMForSkill(
        skill,                          // â† The skill object
        this.config.llm_config || defaultLLM,  // â† Agent's default LLM
        undefined                       // â† Overrides (optional)
      )
    : this.config.llm_config || defaultLLM;
  
  // llmConfig = { provider: 'mistral', model: 'mistral-large-latest', ... }
  
  // 3ï¸âƒ£ EXECUTE WITH SELECTED PROVIDER
  console.log(`ğŸ¤– Executing with ${llmConfig.provider}/${llmConfig.model}`);
  
  const response = await this.llmRouter.execute(
    messages,
    llmConfig    // â† SELECTED LLM CONFIG
  );
  
  return response;
}
```

---

### **STEP 6: LLM Router Selects Provider**

**File:** `src/services/llm/LLMRouter.ts` (Lines 60-97)

```typescript
selectLLMForSkill(
  skill: AgentSkill,              // { name: 'data_analysis', level: 5 }
  defaultLLM: LLMConfig,          // { provider: 'openai', model: 'gpt-4' }
  overrides?: Record<string, LLMConfig>
): LLMConfig {
  
  // ğŸ¯ PRIORITY 1: Skill has its own preferred LLM?
  if (skill.preferred_llm) {
    console.log(`ğŸ¯ Using skill's preferred LLM: ${skill.preferred_llm.provider}`);
    return skill.preferred_llm;
    // Example: { provider: 'mistral', model: 'mistral-large' }
  }

  // ğŸ¯ PRIORITY 2: Is there an override for this skill category?
  const skillType = this.categorizeSkill(skill.name);
  //    â†“
  //    skillName = 'data_analysis'
  //    skillType = 'research'  (because contains 'analysis')
  
  if (overrides && overrides[skillType]) {
    console.log(`ğŸ¯ Using override LLM for ${skillType}: ${overrides[skillType].provider}`);
    return overrides[skillType];
  }

  // ğŸ¯ PRIORITY 3: Use recommended LLM for this skill type
  const recommended = this.recommendLLM(skillType);
  //    â†“
  //    skillType = 'research'
  //    recommended = { provider: 'mistral', model: 'mistral-large-latest', ... }
  
  console.log(`ğŸ¯ Using recommended LLM for ${skillType}: ${recommended.provider}`);
  return recommended;
  
  // NOTE: Currently simplified - would return recommended here
  // In code, it falls back to defaultLLM, but can be enhanced
}
```

---

### **STEP 7: Categorize Skill**

**File:** `src/services/llm/LLMRouter.ts` (Lines 86-97)

```typescript
private categorizeSkill(skillName: string): string {
  const lower = skillName.toLowerCase();  // 'data_analysis'
  
  if (lower.includes('research') || lower.includes('analysis'))  // â† MATCHES!
    return 'research';  // âœ… RETURNS 'research'
    
  if (lower.includes('writ') || lower.includes('content')) 
    return 'writing';
    
  if (lower.includes('code') || lower.includes('programming')) 
    return 'code';
    
  if (lower.includes('summar')) 
    return 'summarization';
    
  if (lower.includes('translat')) 
    return 'translation';
    
  if (lower.includes('conversation') || lower.includes('support')) 
    return 'conversation';
  
  return 'general';
}

// Result: skillType = 'research'
```

---

### **STEP 8: Get Recommendation**

**File:** `src/services/llm/LLMRouter.ts` (Lines 176-229)

```typescript
recommendLLM(taskType: string): LLMConfig {  // taskType = 'research'
  const recommendations: Record<string, LLMConfig> = {
    
    research: {                              // â† MATCHES!
      provider: 'mistral',                   // âœ… SELECTS MISTRAL
      model: 'mistral-large-latest',
      reason: 'Excellent reasoning and analytical capabilities',
      costPerMillion: 8.0
    },
    
    analysis: {
      provider: 'mistral',
      model: 'mistral-large-latest',
      // ...
    },
    
    writing: {
      provider: 'anthropic',  // Claude
      model: 'claude-3-opus-20240229',
      // ...
    },
    
    // ... more recommendations
    
    default: {
      provider: 'openai',
      model: 'gpt-4-turbo-preview',
      // ...
    }
  };

  return recommendations[taskType] || recommendations['default'];
  //     â†“
  //     Returns: { provider: 'mistral', model: 'mistral-large-latest', ... }
}
```

**Result:** `llmConfig = { provider: 'mistral', model: 'mistral-large-latest' }`

---

### **STEP 9: Execute with Selected Provider**

**File:** `src/services/llm/LLMRouter.ts` (Lines 102-141)

```typescript
async execute(
  messages: LLMMessage[],
  config: LLMConfig  // { provider: 'mistral', model: 'mistral-large-latest' }
): Promise<string> {
  
  // 1ï¸âƒ£ GET PROVIDER INSTANCE
  const provider = this.providers.get(config.provider);
  //    â†“
  //    provider = MistralProvider instance
  
  if (!provider) {
    throw new Error(`Provider ${config.provider} not available`);
  }

  // 2ï¸âƒ£ CHECK IF PROVIDER IS AVAILABLE
  const isAvailable = await provider.isAvailable();
  //    â†“
  //    Checks: return !!this.apiKey  (VITE_MISTRAL_API_KEY exists?)
  
  if (!isAvailable) {
    console.log(`âš ï¸ ${config.provider} not available, falling back to OpenAI`);
    // Falls back to OpenAI if Mistral key not configured
    const fallback = this.providers.get('openai');
    return await fallback.chat(messages, { ...config, provider: 'openai' });
  }

  // 3ï¸âƒ£ EXECUTE THE CALL
  console.log(`ğŸ¤– Executing LLM: ${config.provider}/${config.model}`);
  const startTime = Date.now();

  const response = await provider.chat(messages, config);
  //    â†“
  //    Calls: MistralProvider.chat(messages, config)

  const duration = Date.now() - startTime;
  console.log(`âœ… LLM responded in ${duration}ms`);

  return response;
}
```

---

### **STEP 10: Provider Makes API Call**

**File:** `src/services/llm/providers/MistralProvider.ts` (Lines 18-44)

```typescript
async chat(messages: LLMMessage[], config: LLMConfig): Promise<string> {
  if (!this.apiKey) {
    throw new Error('Mistral API key not configured');
  }

  // MAKE API CALL TO MISTRAL
  const response = await fetch('https://api.mistral.ai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${this.apiKey}`  // VITE_MISTRAL_API_KEY
    },
    body: JSON.stringify({
      model: config.model,          // 'mistral-large-latest'
      messages,                     // Conversation history
      temperature: config.temperature ?? 0.7,
      max_tokens: config.maxTokens,
    })
  });

  const data = await response.json();
  return data.choices[0]?.message?.content || '';
  //     â†“
  //     "Based on the quarterly sales data analysis..."
}
```

---

## ğŸ“Š **VISUAL FLOW DIAGRAM**

```
USER MESSAGE: "Analyze quarterly sales data"
    â†“
ChatProcessor
    â†“
OrchestratorAgent
    â†“
Finance Agent.generateResponseWithRAG()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Detect Skill                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ detectSkillFromContext(context)        â”‚
â”‚   â†“                                      â”‚
â”‚ Result: 'data_analysis'                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Select LLM for Skill            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ llmRouter.selectLLMForSkill(skill)      â”‚
â”‚   â†“                                      â”‚
â”‚ 1. Check skill.preferred_llm? âŒ        â”‚
â”‚ 2. Check overrides? âŒ                  â”‚
â”‚ 3. Categorize skill:                    â”‚
â”‚    'data_analysis' â†’ 'research'         â”‚
â”‚ 4. Get recommendation:                  â”‚
â”‚    recommendations['research'] = {      â”‚
â”‚      provider: 'mistral',               â”‚
â”‚      model: 'mistral-large-latest'      â”‚
â”‚    }                                     â”‚
â”‚   â†“                                      â”‚
â”‚ Selected: Mistral Large âœ…              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Execute with Provider           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ llmRouter.execute(messages, config)     â”‚
â”‚   â†“                                      â”‚
â”‚ 1. Get provider instance:               â”‚
â”‚    providers.get('mistral')             â”‚
â”‚    â†’ MistralProvider                    â”‚
â”‚                                          â”‚
â”‚ 2. Check availability:                  â”‚
â”‚    provider.isAvailable()               â”‚
â”‚    â†’ checks VITE_MISTRAL_API_KEY        â”‚
â”‚    â†’ Returns: true/false                â”‚
â”‚                                          â”‚
â”‚ 3. If available:                        â”‚
â”‚    provider.chat(messages, config)      â”‚
â”‚    â†’ Calls Mistral API                  â”‚
â”‚                                          â”‚
â”‚ 4. If NOT available:                    â”‚
â”‚    Fallback to OpenAI                   â”‚
â”‚    â†’ providers.get('openai')            â”‚
â”‚    â†’ Calls OpenAI API instead           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Provider Makes API Call         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MistralProvider.chat(messages, config)  â”‚
â”‚   â†“                                      â”‚
â”‚ fetch('https://api.mistral.ai/v1/...',  â”‚
â”‚   {                                      â”‚
â”‚     method: 'POST',                      â”‚
â”‚     headers: {                           â”‚
â”‚       Authorization: Bearer API_KEY     â”‚
â”‚     },                                   â”‚
â”‚     body: {                              â”‚
â”‚       model: 'mistral-large-latest',    â”‚
â”‚       messages: [...]                   â”‚
â”‚     }                                    â”‚
â”‚   }                                      â”‚
â”‚ )                                        â”‚
â”‚   â†“                                      â”‚
â”‚ Response from Mistral API               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Response: "Analysis of quarterly sales data shows..."
```

---

## ğŸ¯ **SELECTION PRIORITY (EXACT ORDER)**

### **Priority 1: Skill's Preferred LLM** (HIGHEST)

```typescript
// If skill is defined like this:
const skill = {
  name: 'financial_analysis',
  level: 5,
  preferred_llm: {              // â† CUSTOM PREFERENCE
    provider: 'mistral',
    model: 'mistral-large-latest'
  }
};

// Then this LLM is used, period!
// No questions asked, no categorization needed
```

**Example:**
```
Skill: 'financial_analysis'
  â””â”€ Has preferred_llm? âœ… YES
      â””â”€ Use: Mistral Large
      â””â”€ Skip all other checks
```

---

### **Priority 2: Skill Category Override** (HIGH)

```typescript
// If you provide overrides:
const overrides = {
  research: { provider: 'mistral', model: 'mistral-large' },
  writing: { provider: 'anthropic', model: 'claude-opus' },
  code: { provider: 'openai', model: 'gpt-4' }
};

llmRouter.selectLLMForSkill(skill, defaultLLM, overrides);
```

**Example:**
```
Skill: 'data_analysis'
  â†“ Categorize
  Type: 'research'
  â†“ Check overrides
  overrides['research'] exists? âœ… YES
  â””â”€ Use: Mistral Large from override
```

---

### **Priority 3: Smart Recommendation** (MEDIUM)

```typescript
// If no preference or override, use built-in recommendations:

Skill: 'data_analysis'
  â†“ Categorize
  Type: 'research'
  â†“ Get recommendation
  recommendations['research'] = {
    provider: 'mistral',
    model: 'mistral-large-latest',
    reason: 'Excellent reasoning and analytical capabilities'
  }
  â””â”€ Use: Mistral Large
```

**This is the SMART DEFAULT!** âœ¨

---

### **Priority 4: Agent Default LLM** (LOW)

```typescript
// If skill categorization doesn't match any recommendation:

Skill: 'custom_weird_skill'
  â†“ Categorize
  Type: 'general' (no specific match)
  â†“ Get recommendation
  recommendations['default'] OR defaultLLM
  â””â”€ Use: Agent's default (usually OpenAI GPT-4)
```

---

### **Priority 5: Global Fallback** (LOWEST)

```typescript
// If everything fails:

const defaultLLM = {
  provider: 'openai',
  model: 'gpt-4-turbo-preview',
  temperature: 0.7
};

// Always works if OpenAI key is configured
```

---

## ğŸ’¡ **ACTUAL SELECTION EXAMPLES**

### **Example 1: Research Task**

```
User: "Research AI trends in 2024"

Flow:
1. Skill detected: 'research'
2. Categorize: 'research'
3. Recommendation: Mistral Large
4. Check Mistral API key: 
   - If exists â†’ Use Mistral âœ…
   - If not â†’ Fallback to OpenAI âš ï¸
5. Execute: Mistral API call

Console Output:
ğŸ¯ Using recommended LLM for research: mistral
ğŸ¤– Executing LLM: mistral/mistral-large-latest
âœ… LLM responded in 1234ms
```

---

### **Example 2: Writing Task**

```
User: "Write a blog post about our new product"

Flow:
1. Skill detected: 'content_writing'
2. Categorize: 'writing' (contains 'writ')
3. Recommendation: Claude Opus
4. Check Claude API key:
   - If exists â†’ Use Claude âœ…
   - If not â†’ Fallback to OpenAI âš ï¸
5. Execute: Claude API call

Console Output:
ğŸ¯ Using recommended LLM for writing: anthropic
ğŸ¤– Executing LLM: anthropic/claude-3-opus-20240229
âœ… LLM responded in 2100ms
```

---

### **Example 3: Simple Chat**

```
User: "Hello, how are you?"

Flow:
1. Skill detected: 'conversation'
2. Categorize: 'conversation'
3. Recommendation: GPT-3.5 Turbo
4. Check OpenAI key:
   - Exists â†’ Use GPT-3.5 âœ…
5. Execute: OpenAI API call

Console Output:
ğŸ¯ Using recommended LLM for conversation: openai
ğŸ¤– Executing LLM: openai/gpt-3.5-turbo
âœ… LLM responded in 450ms

Cost: $0.0005 (vs $0.01 for GPT-4) = 95% savings! ğŸ’°
```

---

## ğŸ” **PROVIDER AVAILABILITY CHECK**

**Each provider checks if it's configured:**

```typescript
// OpenAI Provider
async isAvailable(): Promise<boolean> {
  return !!this.apiKey;  // âœ… if VITE_OPENAI_API_KEY exists
}

// Mistral Provider
async isAvailable(): Promise<boolean> {
  return !!this.apiKey;  // âœ… if VITE_MISTRAL_API_KEY exists
}

// Claude Provider
async isAvailable(): Promise<boolean> {
  return !!this.apiKey;  // âœ… if VITE_ANTHROPIC_API_KEY exists
}

// Gemini Provider
async isAvailable(): Promise<boolean> {
  return !!this.apiKey;  // âœ… if VITE_GOOGLE_API_KEY exists
}

// Ollama Provider
async isAvailable(): Promise<boolean> {
  // Check if local Ollama server is running
  try {
    await fetch('http://localhost:11434/api/tags');
    return true;  // âœ… if Ollama is running locally
  } catch {
    return false;
  }
}

// Groq Provider
async isAvailable(): Promise<boolean> {
  return !!this.apiKey;  // âœ… if VITE_GROQ_API_KEY exists
}
```

**Current Status:**
- âœ… OpenAI: Available (you're using it)
- âš ï¸ Mistral: Not available (no API key)
- âš ï¸ Claude: Not available (no API key)
- âš ï¸ Gemini: Not available (no API key)
- âš ï¸ Ollama: Not available (not running)
- âš ï¸ Groq: Not available (no API key)

**So it ALWAYS falls back to OpenAI!**

---

## ğŸŠ **SUMMARY**

**Your Question:** "How are they selected?"

**Answer:**

### **Selection Process:**

```
1. Detect skill from user message
   â†“
2. Categorize skill into type
   ('research', 'writing', 'code', etc.)
   â†“
3. Get recommended LLM for that type
   research â†’ Mistral
   writing â†’ Claude
   code â†’ GPT-4
   conversation â†’ GPT-3.5
   â†“
4. Check if provider is available
   (Has API key? Is server running?)
   â†“
5. If available â†’ Use recommended LLM âœ…
   If not â†’ Fallback to OpenAI âš ï¸
   â†“
6. Make API call to selected provider
```

### **Why You Only See OpenAI:**

âš ï¸ **Only OpenAI is configured** in your `.env`

**To enable smart routing:**
```env
# Add these to .env
VITE_MISTRAL_API_KEY=your-mistral-key      # For research/analysis
VITE_ANTHROPIC_API_KEY=your-claude-key     # For writing
VITE_GOOGLE_API_KEY=your-gemini-key        # For translation
VITE_GROQ_API_KEY=your-groq-key            # For speed
```

**Then the system will automatically:**
- âœ… Use Mistral for research/analysis (cheaper, better)
- âœ… Use Claude for writing (best quality)
- âœ… Use GPT-3.5 for chat (95% cheaper)
- âœ… Use Groq for speed (10x faster)

**Smart routing = Better quality + Lower costs!** ğŸš€



