# ğŸ¯ LLM Selection - Quick Reference Guide

## â“ **IS IT ALWAYS GROQ?**

**NO!** The system intelligently selects the best LLM based on multiple factors.

---

## ğŸ”€ **SELECTION HIERARCHY** (Highest to Lowest Priority)

```
1ï¸âƒ£ Skill Preference      â† "This skill needs GPT-4"
         â†“ (if not set)
2ï¸âƒ£ User Task Override    â† "I want Groq for code tasks"
         â†“ (if not set)
3ï¸âƒ£ Organization Policy   â† "Company uses Mistral for analysis"
         â†“ (if not set)
4ï¸âƒ£ Task Type AI Routing  â† System decides based on task
         â†“ (if not available)
5ï¸âƒ£ User Default          â† "My favorite is OpenAI"
         â†“ (if not available)
6ï¸âƒ£ System Default        â† "Use Groq for speed"
         â†“ (if fails)
7ï¸âƒ£ Fallback Chain        â† "Try OpenAI as backup"
```

---

## ğŸ¨ **TASK-TYPE AUTO-ROUTING**

The system **automatically** picks the best provider for each task type:

| Task Type          | Provider    | Model                    | Why?                  |
|--------------------|-------------|--------------------------|----------------------|
| ğŸ’¬ **Chat**        | Groq        | llama3-70b-8192         | 10x faster (800ms)   |
| ğŸ§  **Research**    | Mistral     | mistral-large-latest    | Best reasoning       |
| âœï¸ **Writing**     | Claude      | claude-3-opus           | Most creative        |
| ğŸ’» **Code**        | OpenAI      | gpt-4                   | Best for coding      |
| ğŸŒ **Translation** | Google      | gemini-pro              | Best multilingual    |
| âš¡ **Real-time**   | Groq        | llama3-8b-8192          | Ultra-fast (400ms)   |

---

## ğŸ”§ **HOW USERS CONTROL IT**

### **Option 1: Set Default Provider**
```typescript
User Settings:
  default_provider: 'groq'  // Use Groq for everything
```

### **Option 2: Task-Specific Overrides**
```typescript
User Settings:
  task_overrides: {
    'code': 'openai',      // OpenAI for code
    'research': 'mistral', // Mistral for research
    'chat': 'groq'         // Groq for chat
  }
```

### **Option 3: Skill-Level Preferences**
```typescript
Agent Configuration:
  skills: [
    {
      name: 'analyze_data',
      preferred_llm: {
        provider: 'mistral',
        model: 'mistral-large-latest'
      }
    }
  ]
```

### **Option 4: Provider Priority**
```typescript
User Settings:
  provider_preferences: {
    groq:     { enabled: true,  priority: 1 },
    mistral:  { enabled: true,  priority: 2 },
    openai:   { enabled: true,  priority: 3 },
    anthropic: { enabled: false, priority: 999 }
  }
```

---

## ğŸ“Š **EMBEDDINGS** (Separate from Chat LLMs)

### **ALWAYS OpenAI** ğŸ”’

```
Chat Completions:
  âœ… Groq, Mistral, Claude, OpenAI, Google (dynamic)

Embeddings:
  âœ… OpenAI ONLY (text-embedding-ada-002)
  âŒ Groq doesn't provide embeddings
  âŒ Mistral doesn't provide embeddings
```

**Why?**
- Groq = Chat completions only
- Embeddings need consistency (same model = same dimensions)
- OpenAI ada-002 is industry standard

---

## ğŸ”„ **FALLBACK SYSTEM**

Every provider has an automatic fallback:

```
Groq fails      â†’ OpenAI âœ…
Mistral fails   â†’ OpenAI âœ…
Claude fails    â†’ OpenAI âœ…
Google fails    â†’ OpenAI âœ…
OpenAI fails    â†’ ERROR âŒ (ultimate fallback)
```

**Result:** System rarely fails completely!

---

## ğŸ’¡ **REAL-WORLD EXAMPLES**

### Example 1: Simple Chat
```
User: "Hello, how are you?"

Selection:
  â”œâ”€ No skill preference
  â”œâ”€ Task type: "simple_tasks"
  â”œâ”€ Recommendation: Groq (speed)
  â””â”€ SELECTED: Groq/llama3-70b-8192

Speed: âš¡ 800ms
```

### Example 2: Code Generation
```
Skill: generate_code
Skill Preference: OpenAI/gpt-4

Selection:
  â”œâ”€ Skill has preference âœ…
  â””â”€ SELECTED: OpenAI/gpt-4

Speed: ğŸ’» 2000ms (high quality)
```

### Example 3: User Override
```
User Settings:
  task_overrides: { 'research': 'mistral' }

Task: "Analyze this data"

Selection:
  â”œâ”€ No skill preference
  â”œâ”€ User override: Mistral âœ…
  â””â”€ SELECTED: Mistral/mistral-large-latest

Speed: ğŸ§  3000ms (deep analysis)
```

### Example 4: Groq Unavailable (Fallback)
```
Task: "Quick question"

Selection:
  â”œâ”€ Default: Groq
  â”œâ”€ Try Groq: âŒ API down
  â”œâ”€ Fallback: OpenAI âœ…
  â””â”€ SELECTED: OpenAI/gpt-3.5-turbo

Speed: ğŸ”„ 2500ms (graceful degradation)
```

---

## ğŸ¯ **QUICK ANSWERS**

**Q: Is it always Groq?**  
A: No! It's dynamic based on task, user settings, and availability.

**Q: Can users choose their LLM?**  
A: Yes! Via default provider, task overrides, or provider preferences.

**Q: What about embeddings?**  
A: Always OpenAI. Groq doesn't provide embeddings.

**Q: What if Groq is down?**  
A: Auto-fallback to OpenAI. Users don't notice.

**Q: Can organizations control this?**  
A: Yes! Org can set policies, quotas, and allowed providers.

**Q: Which is fastest?**  
A: Groq (800ms) â†’ Google (2200ms) â†’ OpenAI (2000ms) â†’ Mistral (2500ms) â†’ Claude (3000ms)

**Q: Which is best quality?**  
A: Depends on task:
- Reasoning: Mistral > Claude > OpenAI
- Creative: Claude > Mistral > OpenAI  
- Code: OpenAI > Claude > Mistral
- Speed: Groq > All

---

## ğŸ“ˆ **PROVIDER COMPARISON**

| Provider    | Speed  | Quality   | Cost      | Best For        |
|-------------|--------|-----------|-----------|-----------------|
| **Groq**    | âš¡âš¡âš¡âš¡âš¡ | â­â­â­     | ğŸ’°        | Speed           |
| **Mistral** | âš¡âš¡     | â­â­â­â­â­   | ğŸ’°ğŸ’°ğŸ’°     | Reasoning       |
| **Claude**  | âš¡âš¡     | â­â­â­â­â­   | ğŸ’°ğŸ’°ğŸ’°ğŸ’°   | Creativity      |
| **OpenAI**  | âš¡âš¡âš¡   | â­â­â­â­    | ğŸ’°ğŸ’°       | Code, Fallback  |
| **Google**  | âš¡âš¡âš¡   | â­â­â­â­    | ğŸ’°ğŸ’°       | Multilingual    |

---

## âœ… **BOTTOM LINE**

1. **Default:** Groq (speed)
2. **Smart Routing:** Auto-selects best provider per task
3. **User Control:** Full customization available
4. **Never Fails:** Automatic fallbacks
5. **Embeddings:** Always OpenAI (Groq doesn't have it)

**Your system is intelligent, not hardcoded!** ğŸ¯

